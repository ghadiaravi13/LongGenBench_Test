# Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models

This repository provides the code and data for the paper **"Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models"**.

## Overview

The **Spinning the Golden Thread (SGT)** benchmark is designed to assess the long-form content generation abilities of language models (LMs) for tasks requiring coherent, long-context outputs. Traditional evaluations often focus on short or needle-in-a-haystack style tasks, but **SGT** centers on evaluating models' ability to generate complex, long-form content while adhering to specific constraints.

### [Insert Overview Image Here]
*Insert an image summarizing the benchmark structure or model performance here*

SGT evaluates 10 long-context LMs across several scenarios and prompt types, analyzing their ability to generate content based on varying levels of instruction complexity.

## Installation

Clone this repository:

```bash
git clone https://github.com/your_username/SGT-benchmark.git
cd SGT-benchmark


## Citation

If you find this work useful in your research, please cite our paper:

```bibtex
@article{wu2024spinning,
  title={Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models},
  author={Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqing and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2409.02076},
  year={2024}
}
